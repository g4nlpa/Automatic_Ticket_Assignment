{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding,LSTM,TimeDistributed,Dense,Flatten,Dropout,RepeatVector,GRU,Bidirectional,Permute\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "from keras_preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import concatenate,Activation, dot\n",
    "\n",
    "import os\n",
    "\n",
    "filename='pre_data_dl.xlsx'\n",
    "data_dl=pd.read_excel(filename)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data_dl.head())\n",
    "\n",
    "filename1='pre_data_dl_aug1.xlsx'\n",
    "filename2='pre_data_dl_aug2.xlsx'\n",
    "\n",
    "data_dl_aug1=pd.read_excel(filename1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data_dl_aug1.head())\n",
    "\n",
    "data_dl_aug2=pd.read_excel(filename2)\n",
    "print(data_dl_aug2.head())\n",
    "\n",
    "X = (data_dl[\"Combined Description Cleaned\"])\n",
    "y= (data_dl['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y=pd.get_dummies(data_dl['Assignment group'])\n",
    "  \n",
    "\n",
    "#parameters\n",
    "max_features=10000\n",
    "emb_dim=300\n",
    "batch_size=1024\n",
    "epochs=10\n",
    "out_dim=74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for tokenizer\n",
    "def dfTokenizer(df):\n",
    " tokenizer=Tokenizer(num_words=max_features,char_level=False)\n",
    " tokenizer.fit_on_texts(df)\n",
    " sequences=tokenizer.texts_to_sequences(df)\n",
    " return sequences,tokenizer\n",
    "#tokenization\n",
    "X,tokenizer = dfTokenizer(data_dl[\"Combined Description Cleaned\"]) \n",
    "vocab_size=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size)\n",
    "\n",
    "#function for padding\n",
    "def pad(x, length=None):\n",
    " if length is None:\n",
    "   length=max([len(sentence)  for sentence in x])\n",
    " return pad_sequences(x,maxlen=length,padding='post')\n",
    " \n",
    " #padding \n",
    "X=pad_sequences(X,padding='post')\n",
    "\n",
    "\n",
    "#function for splitting the data\n",
    "def split(X,y):\n",
    " X_train_spl,X_test_spl,y_train_spl,y_test_spl=train_test_split(X,y,test_size=0.2,random_state=123)\n",
    " return X_train_spl,X_test_spl,y_train_spl,y_test_spl\n",
    "#split the data\n",
    "X_train,X_test,y_train,y_test=split(X,y)\n",
    "\n",
    "\n",
    "#configuring the callback\n",
    "early_stopping =  EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    min_delta=0.01, \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "\n",
    "#function for plotting accuracy,loss\n",
    "def plot(model,history):\n",
    " acc = history.history['accuracy']\n",
    " val_acc = history.history['val_accuracy']\n",
    " loss = history.history['loss']\n",
    " val_loss = history.history['val_loss']\n",
    " epochs = range(1, len(acc) + 1)\n",
    " plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    " plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    " plt.title( ' Training and validation accuracy')\n",
    " plt.legend()\n",
    " plt.figure()\n",
    " plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    " plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    " plt.title('Training and validation loss')\n",
    " plt.legend()\n",
    " plt.show()\n",
    " return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_len=X.shape[1]\n",
    "inputs = Input(shape=(inp_len,))\n",
    "#attention with encoder only\n",
    "sequence_input = Input(shape=(inp_len), dtype='int32')        \n",
    "embedded_sequences =Embedding(max_features,emb_dim,input_length=inp_len)(sequence_input)     \n",
    "BILSTM= Bidirectional(LSTM(128, return_sequences=True,dropout=0.2,name=\"bi_lstm_0\",return_state=True))(embedded_sequences)\n",
    "#rp=RepeatVector(max_features)(lstm)\n",
    "\n",
    "#xy = tf.reshape(lstm, [189, 256])\n",
    "#rp=RepeatVector(max_features)(xy)\n",
    "\n",
    "BILSTM,forward_h, forward_c, backward_h, backward_c= Bidirectional \\\n",
    "                                                    (LSTM\n",
    "                                                    (128,\n",
    "                                                     dropout=0.2,\n",
    "                                                     return_state=True,\n",
    "                                                    \n",
    "                                                     return_sequences=True))(BILSTM)\n",
    "                                                   \n",
    "                                                   \n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector,attention_weights = Attention(32)(BILSTM,state_h)\n",
    "fl=Flatten()(context_vector)\n",
    "output = Dense(74, activation='sigmoid')(fl)\n",
    "\n",
    "model_att =tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model_att.summary())\n",
    "\n",
    "\n",
    "#Attention with encoder decoder\n",
    "inp = Embedding(max_features,emb_dim,input_length=inp_len)(inputs)\n",
    "lstm_out = LSTM(64, return_sequences=True)(inp)\n",
    "\n",
    "attention = Dense(1, activation='relu')(lstm_out)\n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(64)(attention)\n",
    "#attention = LSTM(64,return_sequences=False)(attention)\n",
    "attention = Permute([2,1])(attention)\n",
    "\n",
    "combined = concatenate([lstm_out, attention])\n",
    "combined_mul = Flatten()(combined)\n",
    "decode = RepeatVector(64)(combined_mul)\n",
    "\n",
    "decode = LSTM(64, return_sequences=True)(decode)\n",
    "decode=Flatten()(decode)\n",
    "decode = (Dense(74))(decode)\n",
    "\n",
    "decode = Activation('softmax')(decode)\n",
    "\n",
    "model_enc_dec_att = tf.keras.Model(inputs=inputs, outputs=decode)\n",
    "\n",
    "#model= tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\n",
    "model_enc_dec_att.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "print(model_enc_dec_att.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENC_ATT_model(hp): #attention with encoder\n",
    " model = model_att\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])   \n",
    " model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    " return model\n",
    "\n",
    "def ENC_DEC_ATT_model(hp):#attention with encoder decoder\n",
    " model = model_enc_dec_att\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])   \n",
    " model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(kt.tuners.BayesianOptimization):\n",
    "  def run_trial(self, trial, *args, **kwargs):\n",
    "     kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 256, 512,step=256 )\n",
    "     kwargs['epochs'] = trial.hyperparameters.Int('epochs', 5, 10,20)\n",
    "     super(MyTuner, self).run_trial(trial, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_ATT_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/4F\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_ATT are\",best_model_ENC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_ATTModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_ATT.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_ATT,history)\n",
    "\n",
    "scores_ENC_ATT =best_model_ENC_ATT.evaluate(X_test, y_test, verbose=0)\n",
    "scores_ENC_ATT_val = best_model_ENC_ATT.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of ENC_ATT for unaugmented data is :\", (scores_ENC_ATT[1]*100))\n",
    "print(\"Validation Accuracy of ENC_ATT for unagumented data is:\", (scores_ENC_ATT_val[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_ATT for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_ATT for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of ENC_ATT for unaugmented data is :\",np.array(history.history['loss']).mean())\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_DEC_ATT_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/4G\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_DEC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_DEC_ATT are\",best_model_ENC_DEC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_DEC_ATTModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_DEC_ATT.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_DEC_ATT,history)\n",
    "\n",
    "scores_ENC_DEC_ATT =best_model_ENC_DEC_ATT.evaluate(X_test, y_test, verbose=0)\n",
    "scores_ENC_DEC_ATT_val = best_model_ENC_DEC_ATT.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of ENC_DEC_ATT for unaugmented data is :\", (scores_ENC_DEC_ATT[1]*100))\n",
    "print(\"Validation Accuracy of ENC_DEC_ATT for unagumented data is:\", (scores_ENC_DEC_ATT_val[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_DEC_ATT for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_DEC_ATT for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of ENC_DEC_ATT for unaugmented data is :\",np.array(history.history['loss']).mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug1 = (data_dl_aug1[\"Combined Description Cleaned\"])\n",
    "y_aug1= (data_dl_aug1['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y_aug1=pd.get_dummies(data_dl_aug2['Assignment group'])\n",
    "\n",
    "\n",
    "X_aug2 = (data_dl_aug2[\"Combined Description Cleaned\"])\n",
    "y_aug2= (data_dl_aug2['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y_aug2=pd.get_dummies(data_dl_aug2['Assignment group'])\n",
    "\n",
    "#tokenization\n",
    "X_aug1,tokenizer = dfTokenizer(data_dl_aug1[\"Combined Description Cleaned\"]) \n",
    "vocab_size_aug1=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size_aug1)\n",
    "\n",
    "X_aug2,tokenizer = dfTokenizer(data_dl_aug2[\"Combined Description Cleaned\"]) \n",
    "vocab_size_aug2=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size_aug2)\n",
    "\n",
    "#padding\n",
    "X_aug1=pad_sequences(X_aug1,padding='post')\n",
    "X_aug2=pad_sequences(X_aug2,padding='post')\n",
    "y_aug1=y_aug1[0:17586]\n",
    "\n",
    "def split_stratify(X,y):\n",
    " X_train_spl,X_test_spl,y_train_spl,y_test_spl=train_test_split(X,y,test_size=0.2,stratify=y,random_state=123)\n",
    " return X_train_spl,X_test_spl,y_train_spl,y_test_spl\n",
    "\n",
    "X_train_aug1,X_test_aug1,y_train_aug1,y_test_aug1=split_stratify(X_aug1,y_aug1)\n",
    "X_train_aug2,X_test_aug2,y_train_aug2,y_test_aug2=split_stratify(X_aug2,y_aug2)\n",
    "\n",
    "\n",
    "inp_len1=X_aug1.shape[1]\n",
    "print(inp_len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(inp_len1), dtype='int32')        \n",
    "embedded_sequences =Embedding(max_features,emb_dim,input_length=inp_len1)(sequence_input)     \n",
    "BILSTM= Bidirectional(LSTM(128, return_sequences=True,dropout=0.2,name=\"bi_lstm_0\",return_state=True))(embedded_sequences)\n",
    "#rp=RepeatVector(max_features)(lstm)\n",
    "\n",
    "#xy = tf.reshape(lstm, [189, 256])\n",
    "#rp=RepeatVector(max_features)(xy)\n",
    "\n",
    "BILSTM,forward_h, forward_c, backward_h, backward_c= Bidirectional \\\n",
    "                                                    (LSTM\n",
    "                                                    (128,\n",
    "                                                     dropout=0.2,\n",
    "                                                     return_state=True,\n",
    "                                                    \n",
    "                                                     return_sequences=True))(BILSTM)\n",
    "                                                   \n",
    "                                                   \n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector,attention_weights = Attention(32)(BILSTM,state_h)\n",
    "fl=Flatten()(context_vector)\n",
    "output = Dense(74, activation='sigmoid')(fl)\n",
    "\n",
    "model_att1 =tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model_att1.summary())\n",
    "\n",
    "\n",
    "#Attention with encoder decoder\n",
    "inp = Embedding(max_features,emb_dim,input_length=inp_len11)(inputs)\n",
    "lstm_out = LSTM(64, return_sequences=True)(inp)\n",
    "\n",
    "attention = Dense(1, activation='relu')(lstm_out)\n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(64)(attention)\n",
    "#attention = LSTM(64,return_sequences=False)(attention)\n",
    "attention = Permute([2,1])(attention)\n",
    "\n",
    "combined = concatenate([lstm_out, attention])\n",
    "combined_mul = Flatten()(combined)\n",
    "decode = RepeatVector(64)(combined_mul)\n",
    "\n",
    "decode = LSTM(64, return_sequences=True)(decode)\n",
    "decode=Flatten()(decode)\n",
    "decode = (Dense(74))(decode)\n",
    "\n",
    "decode = Activation('softmax')(decode)\n",
    "\n",
    "model_enc_dec_att1 = tf.keras.Model(inputs=inputs, outputs=decode)\n",
    "\n",
    "#model= tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\n",
    "model_enc_dec_att1.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "print(model_enc_dec_att1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9A\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_ATT are\",best_model_ENC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_ATTModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_ATT.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_ATT,history)\n",
    "\n",
    "scores_ENC_ATT_aug1 =best_model_ENC_ATT.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_ENC_ATT_val_aug1 = best_model_ENC_ATT.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of ENC_ATT for level1 augmented data is :\", (scores_ENC_ATT_aug1[1]*100))\n",
    "print(\"Validation Accuracy of ENC_ATT for level1 augmented data is:\", (scores_ENC_ATT_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_ATT for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_ATT for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9F\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_ATT are\",best_model_ENC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_ATTModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_ATT.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_ATT,history)\n",
    "\n",
    "scores_ENC_ATT_aug2 =best_model_ENC_ATT.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_ENC_ATT_val_aug2 = best_model_ENC_ATT.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of ENC_ATT for level2 augmented data is :\", (scores_ENC_ATT_aug2[1]*100))\n",
    "print(\"Validation Accuracy of ENC_ATT for level2 augmented data is:\", (scores_ENC_ATT_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_ATT for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_ATT for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_DEC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9K\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_DEC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_DEC_ATT are\",best_model_ENC_DEC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_DEC_ATTModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_DEC_ATT.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_DEC_ATT,history)\n",
    "\n",
    "scores_ENC_DEC_ATT_aug1 =best_model_ENC_DEC_ATT.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_ENC_DEC_ATT_val_aug1 = best_model_ENC_DEC_ATT.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of ENC_DEC_ATT for level1 augmented data is :\", (scores_ENC_DEC_ATT_aug1[1]*100))\n",
    "print(\"Validation Accuracy of ENC_DEC_ATT for level1 augmented data is:\", (scores_ENC_DEC_ATT_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_DEC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_DEC_ATT for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_DEC_ATT for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_DEC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9H\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_DEC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_DEC_ATT are\",best_model_ENC_DEC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_DEC_ATTModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_DEC_ATT.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_DEC_ATT,history)\n",
    "\n",
    "scores_ENC_DEC_ATT_aug2 =best_model_ENC_DEC_ATT.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_ENC_DEC_ATT_val_aug2 = best_model_ENC_DEC_ATT.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of ENC_DEC_ATT for level2 augmented data is :\", (scores_ENC_DEC_ATT_aug2[1]*100))\n",
    "print(\"Validation Accuracy of ENC_DEC_ATT for level2 augmented data is:\", (scores_ENC_DEC_ATT_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_DEC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_DEC_ATT for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_DEC_ATT for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Attention with encoder for unaugmented data is :\", (scores_ENC_ATT[1]*100))\n",
    "print(\"Accuracy of Attention with encoder & decoder for unagumented data is:\", (scores_ENC_DEC_ATT[1]*100))\n",
    "print(\"Accuracy of Attention with encoder for level1 augmented data is :\", (scores_ENC_ATT_aug1[1]*100))\n",
    "print(\"Accuracy of Attention with encoder for level2 augmented data is :\", (scores_ENC_ATT_aug2[1]*100))\n",
    "print(\"Accuracy of Attention with encoder & decoder for level1 augmented data is :\", (scores_ENC_DEC_ATT_aug1[1]*100))\n",
    "print(\"Accuracy of Attention with encoder & decoder for level2 augmented data is :\", (scores_ENC_DEC_ATT_aug2[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
