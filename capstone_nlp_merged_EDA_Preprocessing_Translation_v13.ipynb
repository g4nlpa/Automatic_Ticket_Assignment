{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDn6EJujF2Ez"
   },
   "source": [
    "# Overview\n",
    "Incident Management and Response is a key component of any IT Service Management Strategy. These are the typical steps involved in the Incident Management Process:\n",
    "- Receipt of the issue \n",
    "- Create a ticket\n",
    "- Review of the ticket by L1/L2 teams\n",
    "- Attempt to resolve the ticket using Standard Operating Procedures by L1/L2\n",
    "- If needed, transfer the ticket to the appropriate L3 team for further review and resolving.\n",
    "\n",
    "\n",
    "# Current ‘Pain’ Points\n",
    "Currently the organization sees these issues in the Incident Ticket Management Process:\n",
    "The process is largely ‘manual’. L1/L2 teams need to spend time to review Standard Operating Procedures (SOPs) before assigning to functional teams. Minimum 25-30% incidents needs to be reviewed for SOPs before ticket assignment. \n",
    "\n",
    "- Minimum 1 FTE effort needed only for incident assignment to L3 teams\n",
    "\n",
    "- Human error - many times the incident gets assigned to the wrong L3 team. So additional effort needed to reassign to the correct team after re-review of the ticket, this not only increases the manual effort needed BUT also leads to customer dis-satisfaction because the customer who opened the ticket is left frustrated because the ticket is in limbo being tossed between various teams before getting to the actual team who can help resolve the issued.\n",
    " \n",
    "\n",
    "# Objective of this Project\n",
    "Create various Machine Learning Models that can help classify incidents and assign them to the right Functional Group. Our objective is to create NLP models that can predict with at least 85% accuracy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "E6igkrhbF2E4",
    "outputId": "9c7632ab-eabe-43d1-bb26-942889c9059b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install googletrans\n",
    "#!pip install textblob\n",
    "!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_lg\n",
    "!pip install -U spacy-lookups-data\n",
    "#!pip install langid\n",
    "!pip install google_trans_new\n",
    "#!pip uninstall googletrans\n",
    "!pip install autocorrect\n",
    "!pip install ftfy\n",
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "!pip install bs4\n",
    "!pip install xgboost\n",
    "!pip install nbconvert[webpdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j46Bdnl0F2E5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "#from langdetect import detect\n",
    "from itertools import cycle\n",
    "#import googletrans\n",
    "#from googletrans import Translator\n",
    "from google_trans_new import google_translator \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "#from textblob import TextBlob\n",
    "#from textblob.translate import NotTranslated\n",
    "import random\n",
    "import operator\n",
    "import math\n",
    "import tqdm\n",
    "import time\n",
    "import spacy\n",
    "import json\n",
    "#import langid\n",
    "from bs4 import BeautifulSoup\n",
    "from string import digits\n",
    "\n",
    "from autocorrect import Speller\n",
    "from ftfy import fix_encoding, fix_text, fix_text_segment, badness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcOgzO11F2E5"
   },
   "source": [
    "### This section below contains  Useful Functions \n",
    "- As we find new functions, we will create them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7Dsf5EiF2E5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"didnt\": \"did not\",\n",
    "\"doesnt\": \"does not\",\n",
    "\"thats\": \"that is\",\n",
    "\"wasnt\": \"was not\",\n",
    "\"weren\": \"were not\",\n",
    "\"theyre\": \"there\",\n",
    "\"dont\": \"do not\",\n",
    "\"cant\": \"cannot\",\n",
    "\"arent\": \"are not\",\n",
    "\"whats\": \"what is\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ALoy7_bF2E6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Most frequently occuring Bi-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fn_word_cloud(column):\n",
    "    \n",
    "    comment_words = ' '\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # iterate through the csv file \n",
    "    for val in column: \n",
    "\n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "\n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        for words in tokens: \n",
    "            comment_words = comment_words + words + ' '\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words) \n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "def removeString(data, regex):\n",
    "    return data.str.lower().str.replace(regex.lower(), ' ')\n",
    "\n",
    "def preprocess(dataset, columnsToPreprocess, regexList):\n",
    "    for column in columnsToPreprocess:\n",
    "        #for regex in regexList:\n",
    "            #dataset[column] = removeString(dataset[column], regex)\n",
    "            dataset[column] = dataset[column].apply(clean_step2)\n",
    "    return dataset\n",
    "\n",
    "def clean_step2(text):\n",
    "#1)remove html tags    \n",
    "   soup=BeautifulSoup(text,\"html.parser\")\n",
    "   text=soup.get_text(separator=\"\")\n",
    "    \n",
    "#2) Remove non-ASCII characters\n",
    "   encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "   text= encoded_string.decode()\n",
    "   \n",
    "#3)lower case    \n",
    "   text=text.lower()\n",
    "   text = ' '.join([w for w in text.split()])\n",
    "\n",
    "#4)remove punctuation       \n",
    "   text = re.sub(r'[^\\w\\s]', '',text) \n",
    "   \n",
    "#5)remove whitespaces\n",
    "   text=\" \".join(text.split())\n",
    " \n",
    "#6)remove  digits  \n",
    "   remove_digits = str.maketrans('', '', digits) \n",
    "   text = text.translate(remove_digits) \n",
    "    \n",
    "#7)remove emails   \n",
    "   text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "   \n",
    "#8)remove hyperlinks\n",
    "   text = re.sub(r'https?:\\/\\/.*\\/\\w*','', text)\n",
    "   \n",
    "#9)remove other characters   \n",
    "   text=text.replace(\"_\",\" \")\n",
    "  \n",
    "   text=text.replace(\"\\\\\",\" \")\n",
    "   return text   \n",
    "\n",
    "def getRegexList():\n",
    "    '''\n",
    "    Adding regex list as per the given data set to flush off the unnecessary text\n",
    "    \n",
    "    '''\n",
    "    regexList = []\n",
    "    regexList += ['From:(.*)\\r\\n']  # from line\n",
    "    regexList += ['Sent:(.*)\\r\\n']  # sent to line\n",
    "    regexList += ['received from:(.*)\\r\\n']  # received data line\n",
    "    regexList += ['received']  # received data line\n",
    "    regexList += ['To:(.*)\\r\\n']  # to line\n",
    "    regexList += ['CC:(.*)\\r\\n']  # cc line\n",
    "    regexList += ['(.*)infection']  # footer\n",
    "    regexList += ['\\[cid:(.*)]']  # images cid\n",
    "    regexList += ['https?:[^\\]\\n\\r]+']  # https & http\n",
    "    regexList += ['Subject:']\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+']  # emails are not required\n",
    "    regexList += ['[0-9][\\-0–90-9 ]+']  # phones are not required\n",
    "    regexList += ['[0-9]']  # numbers not needed\n",
    "    regexList += ['[^a-zA-z 0-9]+']  # anything that is not a letter\n",
    "    regexList += ['[\\r\\n]']  # \\r\\n\n",
    "    regexList += [' [a-zA-Z] ']  # single letters makes no sense\n",
    "    regexList += [' [a-zA-Z][a-zA-Z] ']  # two-letter words makes no sense\n",
    "    regexList += [\"  \"]  # double spaces\n",
    "    \n",
    "    regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
    "    regexList += ['Subject:']\n",
    "    regexList += ['[^a-zA-Z]']\n",
    "\n",
    "    return regexList\n",
    "\n",
    "\n",
    "def lemmatize(stringlist):\n",
    "    processed_all_documents = list()\n",
    "\n",
    "    for desc in stringlist:\n",
    "        word_tokens = word_tokenize(desc) \n",
    "    \n",
    "        filtered_sentence = [] \n",
    "\n",
    "        # Removing Stopwords\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence.append(w) \n",
    "    \n",
    "        # Lemmetization\n",
    "        lemma_word = []\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        for w in filtered_sentence:\n",
    "            word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "            word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "            word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "            lemma_word.append(word3)\n",
    "        words = ' '.join(lemma_word)\n",
    "        processed_all_documents.append(words) \n",
    "    return processed_all_documents\n",
    "\n",
    "\n",
    "# Write a function to apply to the dataset to detect garbage data\n",
    "def detect_garbage(text):\n",
    "    if not badness.sequence_weirdness(text):\n",
    "        # nothing weird, should be okay\n",
    "        return True\n",
    "    try:\n",
    "        text.encode('sloppy-windows-1252')\n",
    "    except UnicodeEncodeError:\n",
    "        # Not CP-1252 encodable, probably fine\n",
    "        return True\n",
    "    else:\n",
    "        # Encodable as CP-1252, Mojibake alert level high\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTx5v18_F2E8"
   },
   "source": [
    "# Milestone 1: Pre-Processing, Data Visualisation and EDA\n",
    "\n",
    "1. Exploring the given Data files\n",
    "2. Understanding the structure of data\n",
    "3. Missing points in data\n",
    "4. Finding inconsistencies in the data\n",
    "5. Visualizing different patterns\n",
    "6. Visualizing different text features\n",
    "7. Dealing with missing values\n",
    "8. Text preprocessing\n",
    "9. Creating word vocabulary from the corpus of report text data\n",
    "10. Creating tokens as required\n",
    "\n",
    "This notebook contains the detailed steps on our path to accomplishing the goals set for Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkCj4RSyF2E-"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NS1T0o9F2E-"
   },
   "source": [
    "##### We manually create the csv file from the excel and use pandas to read the csv.\n",
    "- For some reason when we use the read_excel function, the number of NaN increase to 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "lSB1_1U-F_f4",
    "outputId": "73d17bc0-2d33-4c22-d0b0-268db9984266",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRR_2PDxF2E-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('drive/My Drive/datasets/input_data.csv')\n",
    "#mydata = pd.read_excel(\"datasets/input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "UZsDFGf3F2E_",
    "outputId": "0c4c435f-482a-4fa8-c2f5-04f2de0ec1c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "pSbyIhGOF2E_",
    "outputId": "905633a9-c193-48ef-8efa-2528b2a723f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj61o5Q0F2E_"
   },
   "source": [
    "###### Observation:\n",
    "- There are 8500 records in the dataset\n",
    "- Each Dataset contains 4 columns\n",
    "- The column 'Caller' seems to contain only junk. We will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMRmTDCgF2E_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata = mydata.drop('Caller',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eyIsqh9oF2E_",
    "outputId": "30f0a4aa-b074-43fe-b37f-ed613fc3e3e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "l4iRdzrEF2FA",
    "outputId": "552d80fc-e9ed-4aa1-a687-d22254993626",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing data check #1:\n",
    "mydata.describe(include='all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2k7mTzVF2FA"
   },
   "source": [
    "### We also notice some records with junks characters in Short Description and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTwYgNmmF2FA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata['Short description'] = mydata['Short description'].astype(str)\n",
    "mydata['Description'] = mydata['Description'].astype(str)\n",
    "mydata['Assignment group'] = mydata['Assignment group'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QBcirBjfF2FA",
    "outputId": "1a8acd62-7429-419a-925e-3555f71aa0ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the dataset for garbage data\n",
    "mydata[~mydata.iloc[:,:-1].applymap(detect_garbage).all(1)]\n",
    "mydata['Description'].apply(detect_garbage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QJHSa6uTF2FA",
    "outputId": "ea8383c5-25f3-42d3-8467-7a16ed7d06c0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mydata.iloc[7126]['Short description'])\n",
    "print(mydata.iloc[7969]['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jP99XzN1F2FB",
    "outputId": "b616147a-a6ca-42ee-ab7b-3800895a0611",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take an example of row# 7126 Short Desc and fix it\n",
    "print('Junk text: \\033[1m%s\\033[0m\\nFixed text: \\033[1m%s\\033[0m' % (mydata['Short description'][7126], \n",
    "                                                                        fix_text(mydata['Short description'][7126])))\n",
    "\n",
    "# List all mojibakes defined in ftfy library\n",
    "print('\\nMojibake Symbol RegEx:\\n', badness.MOJIBAKE_SYMBOL_RE.pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "N-jyM3XAF2FB",
    "outputId": "333ce016-9f06-428d-ac3c-5ded899857c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanitize the dataset from Mojibakes\n",
    "mydata['Short description'] = mydata['Short description'].apply(fix_text_segment)\n",
    "mydata['Description'] = mydata['Description'].apply(fix_text)\n",
    "\n",
    "# Visualize that row# 7126\n",
    "mydata.iloc[7126,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YNc9tD8F2FB"
   },
   "source": [
    "#  Observation\n",
    "- There seem to a few invalid values in Sort Description & Description.\n",
    "- On further checking we find that they can be converted to valid non english alphabets using ftfy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mjokeWTzF2FB",
    "outputId": "1d205253-3b6f-43e6-b2cf-c504965ee34d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.iloc[1081,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "qiOs1sg7F2FB",
    "outputId": "f90e4715-8145-4ede-ee69-9c1b0a75a5ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mydata.query('Description == \"\"')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "R5V2yIPxF2FC",
    "outputId": "3095e7ea-9d71-4b56-c3e4-2433157cc370",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Missing data check #2 : \n",
    "## Are there any null values\n",
    "mydata.isna().apply(pd.value_counts)\n",
    "## Short Description contains 2 nulls and Description contains 1 null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "LXCn0ZTOF2FC",
    "outputId": "86353aae-4d16-4b75-cde9-a82f645374e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "UwiIb3cxF2FC",
    "outputId": "aed18315-70f8-4188-a7e3-92512be2d8ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This method is useful because it shows count, mean, and standard deviation along with the 5 point summary\n",
    "mydata.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvPpyGRlF2FC"
   },
   "source": [
    "#### Number of classes in the Assignment Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RPpOy9mcF2FC",
    "outputId": "87130e80-b7cb-4a17-a5b7-e430b8043a6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(mydata['Assignment group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbVi7bcjF2FC"
   },
   "source": [
    "#### Assignment Group Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "aT-DxmxHF2FD",
    "outputId": "7c964a69-a1d4-4ab0-c471-7a7ba568ed28",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata['Assignment group'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brikogmNF2FD"
   },
   "source": [
    "###### Assignment Group Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "ka5PIei4F2FD",
    "outputId": "07b1f872-bb70-41ad-e1c4-c1c65e65245b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_assignment_group_dist = mydata['Assignment group'].value_counts().reset_index()\n",
    "df_assignment_group_dist['percentage'] = (df_assignment_group_dist['Assignment group']/df_assignment_group_dist['Assignment group'].sum())*100\n",
    "df_assignment_group_dist.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "MxzxOSitF2FD",
    "outputId": "705adb08-b7dc-48b1-ea9c-44d78be36efd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot to visualize the percentage data distribution across different groups\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20,5))\n",
    "order = mydata[\"Assignment group\"].value_counts().index\n",
    "\n",
    "ax = sns.countplot(x=\"Assignment group\", data=mydata, order=order, linewidth=2,\n",
    "                  edgecolor = \"k\"*len(order), palette='Set1')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(format(p.get_height()/len(mydata.index)*100, '.2f')+\"%\"), \n",
    "              (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'bottom',\n",
    "              rotation=90, xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUoONe4uF2FD"
   },
   "source": [
    "# Observation\n",
    "- Group 0 has the most entries - this is expected because we guess Grp_0 is L1 - so gets the most tickets and also resolves them directly based on SOPs (Standard Operating Procedures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ly3sHf3F2FD"
   },
   "source": [
    "#### Top 20 Assignment groups with highest number of tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "x6TNV5m0F2FD",
    "outputId": "0b08ec30-cf63-4001-e50c-99fb8a611369",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_top_20 = mydata['Assignment group'].value_counts().nlargest(20).reset_index()\n",
    "df_top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "k9VDByLhF2FE",
    "outputId": "6860411e-e708-423b-c1a1-37c1c44ea05e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green']\n",
    "i = -1\n",
    "def getCycledColor():\n",
    "    global i, colors\n",
    "    if i < len(colors) -1:\n",
    "        i = i + 1\n",
    "        return colors[i]\n",
    "    else:\n",
    "        i = -1\n",
    "plt.figure(figsize=(12,6))\n",
    "bars = plt.bar(df_top_20['index'],df_top_20['Assignment group'], facecolor=getCycledColor())\n",
    "plt.title('Top 20 Assignment groups with highest number of Tickets')\n",
    "plt.xlabel('Assignment Group')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Number of Tickets')\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x(), yval + .005, yval)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KqEoGwWF2FE"
   },
   "source": [
    "#### Bottom 20 Assignment groups with least number of tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "4WZDAdZZF2FE",
    "outputId": "e25b1d5f-f19f-4ea0-a81a-2703b7bda437",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bottom_20 = mydata['Assignment group'].value_counts().nsmallest(20).reset_index()\n",
    "df_bottom_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "XuAd-6h6F2FE",
    "outputId": "0be87e61-c951-4434-b7fa-78d39ab8c1f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "bars = plt.bar(df_bottom_20['index'],df_bottom_20['Assignment group'], color='green')\n",
    "plt.title('Bottom 20 Assignment groups with small number of Tickets')\n",
    "plt.xlabel('Assignment Group')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Number of Tickets')\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x(), yval + .005, yval)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPXKJM_FF2FE"
   },
   "source": [
    "#### Distribution of tickets counts in various bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "c77ivt9DF2FE",
    "outputId": "0cc3cdd9-4d15-4028-c7d4-63b6eba0a9dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bins = pd.DataFrame(columns=['Description','Ticket Count'])\n",
    "one_ticket = {'Description':'1 ticket','Ticket Count':len(df_assignment_group_dist[df_assignment_group_dist['Assignment group'] < 2])}\n",
    "_2_5_ticket = {'Description':'2-5 ticket',\n",
    "              'Ticket Count':len(df_assignment_group_dist[(df_assignment_group_dist['Assignment group'] > 1)& (df_assignment_group_dist['Assignment group'] < 6) ])}\n",
    "_10_ticket = {'Description':' 6-10 ticket',\n",
    "              'Ticket Count':len(df_assignment_group_dist[(df_assignment_group_dist['Assignment group'] > 5)& (df_assignment_group_dist['Assignment group'] < 11)])}\n",
    "_10_20_ticket = {'Description':' 11-20 ticket',\n",
    "              'Ticket Count':len(df_assignment_group_dist[(df_assignment_group_dist['Assignment group'] > 10)& (df_assignment_group_dist['Assignment group'] < 21)])}\n",
    "_20_50_ticket = {'Description':' 21-50 ticket',\n",
    "              'Ticket Count':len(df_assignment_group_dist[(df_assignment_group_dist['Assignment group'] > 20)& (df_assignment_group_dist['Assignment group'] < 51)])}\n",
    "_51_100_ticket = {'Description':' 51-100 ticket',\n",
    "              'Ticket Count':len(df_assignment_group_dist[(df_assignment_group_dist['Assignment group'] > 50)& (df_assignment_group_dist['Assignment group'] < 101)])}\n",
    "_100_ticket = {'Description':' >100 ticket',\n",
    "              'Ticket Count':len(df_assignment_group_dist[(df_assignment_group_dist['Assignment group'] > 100)])}\n",
    "#append row to the dataframe\n",
    "#append row to the dataframe\n",
    "df_bins = df_bins.append([one_ticket,_2_5_ticket,_10_ticket,\n",
    "                          _10_20_ticket,_20_50_ticket,_51_100_ticket,_100_ticket], ignore_index=True)\n",
    "\n",
    "df_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "YpOjA0JjF2FF",
    "outputId": "1efac465-37b7-471a-fe99-e624e9c0b2fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(df_bins['Ticket Count'],labels=df_bins['Description'],autopct='%1.1f%%', startangle=15, shadow = True);\n",
    "plt.title('Assignment Groups Distribution')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rakdob-uF2FF"
   },
   "source": [
    "# Fetch wordcount for each Ticket in its raw state \n",
    "- (so far we have handled only junk characters and replaced any Nans with empty strings)\n",
    "- We will merge the Short and Description fields just to perform EDA on the tickets. We will redo this (merge step) later after translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "NUos_AQ2F2FF",
    "outputId": "3f9aab77-6222-4890-8952-9c6e5ad956d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging  the 2 preprocessed columns to a single column without duplicate words\n",
    "mydata['Raw Combined description'] = mydata['Short description'] .map(str) + ' ' +  mydata['Description'].map(str)    \n",
    "mydata['Raw Combined description'] = mydata['Raw Combined description'].apply(lambda x: ' '.join(pd.unique(x.split()))) \n",
    "\n",
    "mydata['raw_word_count'] = mydata['Raw Combined description'].apply(lambda x: len(str(x).split(\" \")))\n",
    "mydata[['Raw Combined description','raw_word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "n4qZtZOoF2FF",
    "outputId": "3d748dfa-77c9-4fe8-ef3c-09ba87f542ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mydata.query('Description == \"\"')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HQdPZSYcF2FF",
    "outputId": "9bdca26c-f325-4b27-8ec5-86a58e321384",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Identify common words\n",
    "freq = pd.Series(' '.join(mydata['Raw Combined description']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QV4qljxF2FG"
   },
   "source": [
    "# We now will list the most common words used - in this round will NOT remove any stop words - we will do that later and repeat this step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1010
    },
    "id": "cjQi_iLYF2FG",
    "outputId": "28b66c81-28e3-4e97-c979-fbcb7e8b7d3a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_df = pd.DataFrame(freq)\n",
    "top_df.reset_index(level=0, inplace=True) \n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "#Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "R8miegJkF2FG",
    "outputId": "ac88ed13-651b-4da4-a9f8-f4d2e8b45e5f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Identify uncommon words\n",
    "freq1 =  pd.Series(' '.join(mydata['Raw Combined description']).split()).value_counts()[-20:]\n",
    "freq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "id": "DVsP1SMFF2FG",
    "outputId": "2d3e3fd3-fe6f-4824-8160-f065480c1fbe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_df = pd.DataFrame(freq1)\n",
    "top_df.reset_index(level=0, inplace=True) \n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "#Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXl7lkUOF2FG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = mydata\n",
    "#data[\"Assignment group\"] = data[\"Assignment group\"].apply(lambda x: x.replace(\"GRP_\", \"\"))\n",
    "#data[\"Assignment group\"] = data[\"Assignment group\"].astype(int)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrUjm2KDF2FH"
   },
   "source": [
    "##### Now let's cleanup the null values in Short Description and Description fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "JN0adhlhF2FH",
    "outputId": "b463359c-1588-4fa1-8f05-0a6bc4bf7847",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata[mydata['Description'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "9svdyWkoF2FH",
    "outputId": "6f45afd7-0393-42a0-b387-130e0f893180",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata[mydata['Short description'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpWRl4OoF2FH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replace NaN values in Short Description and Description columns\n",
    "#mydata['Short description'] = mydata['Short description'].replace(np.nan, '', regex=True)\n",
    "#mydata['Description'] = mydata['Description'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "05gw7dUaF2FH",
    "outputId": "af861964-f7d9-4106-ef6a-2a04d075cecf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "blXfcaqoF2FI",
    "outputId": "bbcf7ba6-a7de-4a5e-e064-80063e6c50de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yTzvJMZuF2FI",
    "outputId": "3c1abf2d-8bd2-4211-cdf4-8f40c129ad47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.iloc[1178,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "dPH4ameeF2FI",
    "outputId": "33d14025-dec0-4d1e-e4cb-da4f7cfedc33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "S49C0vupF2FI",
    "outputId": "fe911e3a-4c88-4bd5-a01f-3dfcc1cb4a18",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFebtIv5F2FI"
   },
   "source": [
    "##### Now let's merge the Short Description and Description to a new field - Combined Description . This will help us create a rich corpus\n",
    "- Please note we are doing this now to help us with the word cloud step. \n",
    "- We will repeat this step later again if we find non english characters that we need to translate. This step will be repeated after the translation is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEqyPEOUF2FJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata2 = mydata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DXy-p15qF2FJ",
    "outputId": "60085443-b24f-4142-d1b3-e146da32a3ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.iloc[1178,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0fvZwGORF2FJ",
    "outputId": "cfa9e205-5ca6-450f-bfa1-4689517dad35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging  the 2 preprocessed columns to a single column without duplicate words\n",
    "mydata2['Combined description'] = mydata2['Short description'] .map(str) + ' ' +  mydata2['Description'].map(str)\n",
    "                    \n",
    "mydata2['Combined description'] = mydata2['Combined description'].apply(lambda x: ' '.join(pd.unique(x.split()))) \n",
    "   \n",
    "#testing on single entry\n",
    "print(mydata2.iloc[279]['Short description'])\n",
    "print(mydata2.iloc[279]['Description'])\n",
    "print(mydata2.iloc[279]['Combined description']) \n",
    "print(mydata2.iloc[7126]['Short description'])\n",
    "print(mydata2.iloc[7126]['Combined description'])\n",
    "print(mydata2.iloc[7969]['Description'])\n",
    "print(mydata2.iloc[7969]['Combined description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_kQONbFF2FJ"
   },
   "source": [
    "# now let's print the word cloud\n",
    "- Word clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of al data (such as a speech blog post, or database), the bigger and bolder it appears in the word cloud.\n",
    "\n",
    "- A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it’s mentioned within a given text and the more important it is.\n",
    "\n",
    "- Also known as tag clouds or text clouds, these are ideal ways to pull out the most pertinent parts of textual data, from blog posts to databases. They can also help business users compare and contrast two different pieces of text to find the wording similarities between the two. \n",
    "\n",
    "#### We will print the word cloud for the top 5 groups - GRP_0, GRP_8, GRP_24, GRP_12, GRP_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "-OUBtXnMF2FJ",
    "outputId": "4f28f72f-1698-470b-984f-453f10b8133b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata2[mydata2['Assignment group']=='GRP_0'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "rOi1Pf89F2FK",
    "outputId": "7836fdad-98c8-44c2-883e-0fa6f1dcdd0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata2[mydata2['Assignment group']=='GRP_8'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "INS7EYVlF2FK",
    "outputId": "7c6a3d44-5c86-4055-c1cf-256d8171c5d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata2[mydata2['Assignment group']=='GRP_12'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "DJ4HRZPWF2FK",
    "outputId": "a4b30d06-4656-48b0-9eca-c6e3269c0ee2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata2[mydata2['Assignment group']=='GRP_9'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "0E-lXmsuF2FK",
    "outputId": "17db8070-97a2-4f34-a625-8a48aa84a069",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata2[mydata2['Assignment group']=='GRP_24'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "IFjuCwgoF2FK",
    "outputId": "6e8bbe01-4f88-488c-be84-f7e98508c202",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata2[mydata2['Assignment group']=='GRP_30'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEnkB6RXF2FL"
   },
   "source": [
    "#### Observation\n",
    "- Many non english words in GRP_24\n",
    "- In GRP_30 there are many special characters \n",
    "#### Let us take a quick diversion to look into this further a little bit more\n",
    "- We will first run the google's language detect in multi-threaded fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "qp4HmSFCF2FL",
    "outputId": "23a4a332-2b94-4555-a651-bcc301f9f4f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(20) # Threads\n",
    "\n",
    "def request(text):\n",
    "    #lang = \"zh\"\n",
    "    t = google_translator(timeout=20)\n",
    "#    print(\"Detect Text \" + text)\n",
    "    detect_text = t.detect(text)\n",
    "    #print(detect_text)\n",
    "    return detect_text\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "      time1 = time.time()\n",
    "      #with open(\"datasets/ShortDescriptions.txt\",'r',encoding='utf-8') as f_p:\n",
    "      # texts = f_p.readlines()\n",
    "      #print(texts)\n",
    "      data = mydata2['Short description'].values.tolist()\n",
    "      try:\n",
    "          results = pool.map(request, data)\n",
    "          #print(results)\n",
    "      except Exception as e:\n",
    "          raise e\n",
    "      pool.close()\n",
    "      pool.join()\n",
    "\n",
    "      time2 = time.time()\n",
    "      print(\"Detecting %s Short Desciptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jubBAc-xF2FL"
   },
   "source": [
    "#### We will load the results to a dataframe and print the last few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "gL6xA2nuF2FL",
    "outputId": "02e4f56c-172c-427b-ec29-f2c84a6fa0f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame (results,columns=['language', 'language name'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "Vk0kZbwkF2FL",
    "outputId": "cd1b681b-cf9a-4de6-b28f-08ce5b3a53d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata2.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "v789uyDzF2FL",
    "outputId": "8afaad8e-d426-4791-ba3b-cba60e4ad563",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdlFjjOSF2FL"
   },
   "source": [
    "#### Counts by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Hz1CMBZVF2FM",
    "outputId": "5c0bcb3c-595e-4979-9ce3-335e125bbec9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCJ2NE-hF2FM"
   },
   "source": [
    "### We will graph the distribution of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "S36UJeW8F2FM",
    "outputId": "dfaa1fc8-b063-403e-90bc-f7eed400198b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycol = cycle('bgrcmk')\n",
    "x = df[\"language\"].value_counts()\n",
    "x=x.sort_index()\n",
    "plt.figure(figsize=(10,6))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Distribution of text by language\")\n",
    "plt.ylabel('number of records')\n",
    "plt.xlabel('Language')\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqQMdMuHF2FM"
   },
   "source": [
    "# Observation\n",
    "- Most items are in English followed by German\n",
    "- The other languages are in low single digits - a could in low 2 digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "3_np8RUWF2FM",
    "outputId": "31a41147-f0c6-4193-e3fa-34f405b240b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata2[mydata2.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxATSXR9F2FM"
   },
   "source": [
    "#### We will merge the language columns into the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EYW3ioxAF2FN",
    "outputId": "883aff6f-f3cb-43a8-a618-1e7bbfaff1ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4oHF9UULF2FN",
    "outputId": "ec3cc37c-62c2-4a80-e284-73ead921eeb8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata3=mydata.copy()\n",
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "gxDz-w_dF2FN",
    "outputId": "6c51fb9e-cc65-4f26-8deb-51b93b8df4fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hRMLhHAwF2FN",
    "outputId": "da02161d-51bd-451b-c795-675b9dbea1b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata = mydata2.join(df)\n",
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qho007n3F2FN"
   },
   "source": [
    "#### Observation\n",
    "- It is interesting to see row 8498 . Short description is in Portugese but Description is in English. \n",
    "- The Combine Description gets interpreted as English (we ran the detect alogrithm separately to confirm this)\n",
    "\n",
    "#### This is the reason we decided to translate Short description and Description independently and then merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "3m5f0P3HF2FO",
    "outputId": "88880443-954e-45d2-9cfd-bec1763cef24",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "FvQMBkAuF2FO",
    "outputId": "ae84cc73-cdba-405f-d195-7963d80d80ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata[mydata['Combined description'] == \"\"]\n",
    "#empty_space[['Raw Combined description', 'Assignment group']]\n",
    "empty_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbKjFzKFF2FO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns for cleaning\n",
    "#columnsToPreprocess = ['Short description', 'Description']\n",
    "#columnsToPreprocess = ['Combined description', 'Short description', 'Description']\n",
    "# Create list of regex to remove sensitive data\n",
    "# Clean dataset and remove sensitive data\n",
    "#mydata = preprocess(mydata, columnsToPreprocess, getRegexList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "wCs9zRNhF2FO",
    "outputId": "8d3a8d61-d9c6-420d-cba0-0dc45a04e3f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "kabpR8bJF2FO",
    "outputId": "2e470d18-bb1b-4e1b-aa9a-6145ea5d1dcb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata[mydata['Combined description'] == \"\"]\n",
    "empty_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3Kx6mHbF2FP"
   },
   "source": [
    "### We will attempt Translation into english all the non-english rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2DgYvzSF2FP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(20) # Threads\n",
    "\n",
    "def request(text):\n",
    "    t = google_translator(timeout=20)\n",
    "    translate_text = t.translate(text, lang_tgt='en')\n",
    "    return translate_text\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "      time1 = time.time()\n",
    "      data = mydata['Short description'].values.tolist()\n",
    "      try:\n",
    "          results = pool.map(request, data)\n",
    "          #print(results)\n",
    "      except Exception as e:\n",
    "          raise e\n",
    "      pool.close()\n",
    "      pool.join()\n",
    "\n",
    "      time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zmtSzVzPF2FP",
    "outputId": "6889817a-afe3-43f0-a2fe-7cfaabebe83f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Translating %s Short Descriptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "2AMhpGDGF2FP",
    "outputId": "26e4efb6-03a7-423e-ba3c-27d171c22fe3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame (results,columns=['Translated Short description'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "IqCsWIaEF2FP",
    "outputId": "4f740d38-91d9-4343-dddf-6af862689d0a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Missing data check #2 : \n",
    "## Are there any null values\n",
    "mydata.isna().apply(pd.value_counts)\n",
    "## Short Description contains 2 nulls and Description contains 1 null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "PFEK7iiKF2FQ",
    "outputId": "6839fc1d-a1af-4c20-9d52-19758601c570",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "knB7wdYlF2FQ",
    "outputId": "c77fb465-08f5-429d-d338-67f6bcbffe70",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysOkhAR5F2FQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata[mydata['Combined description'] == \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEtr4zL8F2FQ"
   },
   "source": [
    "#### We will merge the Translated Short description column into the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "lKfL0oRYF2FQ",
    "outputId": "27bbcb9d-c211-4336-8e1f-3f8fb89c6ecd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata2 = mydata\n",
    "mydata = mydata.join(df)\n",
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RWcEVeE5F2FR",
    "outputId": "95ed3a1a-a98b-40c0-fe0e-d009c17b6187",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(20) # Threads\n",
    "\n",
    "def request(text):\n",
    "    t = google_translator(timeout=25)\n",
    "    translate_text = t.translate(text.strip(), lang_tgt='en')\n",
    "    return translate_text\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "      time1 = time.time()\n",
    "      data = mydata['Description'].values.tolist()\n",
    "      try:\n",
    "          results = pool.map(request, data)\n",
    "          #print(results)\n",
    "      except Exception as e:\n",
    "          raise e\n",
    "      pool.close()\n",
    "      pool.join()\n",
    "\n",
    "      time2 = time.time()\n",
    "      print(\"Translating %s Descriptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "liWNPmBRF2FR",
    "outputId": "6e2fa876-5994-42f9-843f-de6eabb0199f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " print(\"Translating %s Descriptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "LZ50bZyLF2FR",
    "outputId": "467f3bce-43c0-4135-b7de-862c18af39e7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame (results,columns=['Translated Description'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "gyHOz2pfF2FR",
    "outputId": "7c86dcd6-9777-44a0-bb7d-379cf7f147f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Missing data check #2 : \n",
    "## Are there any null values\n",
    "mydata.isna().apply(pd.value_counts)\n",
    "## Short Description contains 2 nulls and Description contains 1 null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "oXXPuYCKF2FR",
    "outputId": "1a0f82d1-8812-4f78-cfa2-141c8df54555",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiQLtrCmF2FR"
   },
   "source": [
    "#### We will merge the Combined Description column into the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "bFMe6jdFF2FR",
    "outputId": "34cb114e-5355-4de6-a0aa-cb3e0121c9f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata2 = mydata\n",
    "mydata = mydata.join(df)\n",
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq6OHHK5F2FS"
   },
   "source": [
    "### We will merge the Short description and Description Columns again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nGP4GifdF2FS",
    "outputId": "cb801a24-ce44-4e27-ccf4-634ca2f4eb26",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging  the 2 preprocessed columns to a single column without duplicate words\n",
    "mydata['Combined description'] = mydata['Translated Short description'] .map(str) + ' ' +  mydata['Translated Description'].map(str)\n",
    "                    \n",
    "mydata['Combined description'] = mydata['Combined description'].apply(lambda x: ' '.join(pd.unique(x.split()))) \n",
    "   \n",
    "#testing on single entry\n",
    "print(mydata.iloc[279]['Short description'])\n",
    "print(mydata.iloc[279]['Description'])\n",
    "print(mydata.iloc[279]['Combined description']) \n",
    "print(mydata.iloc[7126]['Short description'])\n",
    "print(mydata.iloc[7126]['Combined description'])\n",
    "print(mydata.iloc[7969]['Description'])\n",
    "print(mydata.iloc[7969]['Combined description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "_00DlZihF2FS",
    "outputId": "d2d62b73-685e-4912-9050-a9738940a0c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZZ25OSPxF2FS",
    "outputId": "c2b00a3c-5323-4d06-ddd3-44b46ab52397",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = mydata.iloc[8499]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "uj9sYZAXF2FS",
    "outputId": "eeb8b765-5301-423e-88ff-22041add0dd1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "qdIQwLdIF2FS",
    "outputId": "c6309fda-ff3c-47ca-9669-dc6df8a560c0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = mydata.iloc[1954]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "FLQTzFPFF2FT",
    "outputId": "65dafeaa-8cf6-4f3e-e0ce-d8f0ae3dd4a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = mydata.query('Description == \"\"')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "Y3cTtrExF2FT",
    "outputId": "a193b0f4-b49d-496b-a7af-a1b40953092c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata[mydata['Combined description'] == \"\"]\n",
    "empty_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "LXtbG6lzF2FT",
    "outputId": "823bc7da-839c-4a21-aca0-c9833ea07d82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.iloc[1178,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK1Ac8NwF2FT"
   },
   "source": [
    "# Preprocessing\n",
    "### We will now attempt to remove unwanted text in the columns of interest to us: \n",
    "- Combined description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqJDTBGNF2FT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns for cleaning\n",
    "#columnsToPreprocess = ['Short description', 'Description']\n",
    "columnsToPreprocess = ['Combined description']\n",
    "# Create list of regex to remove sensitive data\n",
    "# Clean dataset and remove sensitive data\n",
    "mydata = preprocess(mydata, columnsToPreprocess, getRegexList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 943
    },
    "id": "0tepO5w4F2FT",
    "outputId": "8a38251b-d8f7-4150-d720-0a63a01d5f8b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "gwPvALqjF2FT",
    "outputId": "b855e08c-9d08-471a-994d-58b99ce9fac8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata[mydata['Combined description'] == \"\"]\n",
    "empty_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UoSUqohF2FU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.at[8043,'Combined description']=mydata.iloc[8043]['Description']\n",
    "mydata.at[8072,'Combined description']=mydata.iloc[8072]['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T85mhOSAF2FU"
   },
   "source": [
    "# Observation\n",
    "- For some reason the translation got confused and marked these as Greek and also translated to Greek . We are manually fixing these back to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InFOxG3QF2FU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#expand contractions\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "# Expanding Contractions in the reviews\n",
    "mydata['Combined description']=mydata['Combined description'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "7_9Y-gGgF2FU",
    "outputId": "7e8080d8-b5b0-4995-eae7-84c0f80b8f09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjJf5Rl7F2FU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spell =Speller('en', fast=True)       #Speller(fast=True) for faster but less accurate correctiondata\n",
    "mydata['Combined description']=[' '.join([spell(i) for i in x.split()]) for x in mydata['Combined description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "id": "LJHlsBoeF2FU",
    "outputId": "dc5d0aee-06e2-4d4b-e41d-8063a821ca76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YKQCD38RF2FV",
    "outputId": "c35950fe-e38f-4289-cce8-4d781bc4e3ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.iloc[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "E86o2lmpF2FV",
    "outputId": "08bfa6ee-055b-47ce-a324-dd0d292c52e4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove non english words\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "Word = list(set(words.words()))\n",
    "mydata['Combined description'] = [\" \".join(w for w in nltk.wordpunct_tokenize(x) \n",
    "                       if w.lower() in Word or not w.isalpha()) \n",
    "                       for x in mydata['Combined description']]\n",
    "\n",
    "#testing one a single entry\n",
    "print(mydata.iloc[255]['Combined description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "5kQAJUypF2FV",
    "outputId": "f266b2ca-83bc-4785-eeae-6b2ecd0e0837",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = mydata[mydata['Combined description'] == \"\"]\n",
    "print(rows.index)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmajDYjTF2FV"
   },
   "source": [
    "# Observation\n",
    "- For some reason the translation and remove regex alogrithms got confused and did not do their job for these rows properly . We are manually fixing these entries\n",
    "- We can drop rows  2045, 2070, 2192 because they look like junk BUT at this point we will just keep them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ui5_MxwF2FV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row_no in rows.index:\n",
    "    mydata.at[row_no,'Combined description']=mydata.iloc[row_no]['Raw Combined description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "lGpTBhg5F2FV",
    "outputId": "9a3525e0-66d7-4ba5-fbf6-fcafbc4507b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = mydata[mydata['Combined description'] == \"\"]\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plybVv86F2FV"
   },
   "source": [
    "# We are creating the dataset required for Deep Learning first\n",
    "- This data set contains all the stop words too which are important for DL algorithms for Context retention\n",
    "- This dataset will not contain the lemmatization  which is to follow later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "ZavIpr7jF2FW",
    "outputId": "5d590776-6e29-4f8b-e72e-2ba32d9fbf56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata_dl = mydata[['Combined description', 'Assignment group']]\n",
    "#mydata_dl = mydata_dl.drop(mydata_dl.index[mydata_dl[\"Combined description\"] == ''])\n",
    "mydata_dl = mydata_dl.rename(columns = {'Combined description':'Combined Description Cleaned'}) \n",
    "mydata_dl = mydata_dl.reset_index(drop=True)\n",
    "mydata_dl.to_csv('drive/My Drive/datasets/input_data_after_preprocessing_for_dl.csv') \n",
    "mydata_dl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "08efz8RlF2FW",
    "outputId": "10d166bf-d834-4c18-9b43-04b836c73358",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uCwlPCbWF2FW",
    "outputId": "143c7735-eef8-4a10-8df0-3600e23e6667",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata['CombinedWordCount'] = [len(desc.split(' ')) for desc in mydata['Combined description']]\n",
    "mydata.head()\n",
    "wordCount_before_lemmatization = mydata['CombinedWordCount'].sum()\n",
    "print(\"Total Corpus Word Count before lemmatization: \", wordCount_before_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "id": "w89EjbMPF2FW",
    "outputId": "698483ed-2781-499a-a74a-48d8645278b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uyjyrFbpF2FW",
    "outputId": "927ae0a5-5ac6-48da-9e1b-a996efdcb211",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('please')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Woum2lbUF2FW",
    "outputId": "58fe2081-6f90-4316-b621-06ead2de8da4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "mydata['Combined Description Cleaned'] = lemmatize(mydata['Combined description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RwZxdT7sF2FX",
    "outputId": "49e0c92d-be93-40ec-b669-6ab6516f0088",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata['CombinedWordCountCleaned'] = [len(desc.split(' ')) for desc in mydata['Combined Description Cleaned']]\n",
    "wordCount_after_lemmatization = mydata['CombinedWordCountCleaned'].sum()\n",
    "print(\"Total Corpus Word Count after lemmatization: \", wordCount_after_lemmatization)\n",
    "print(\"Max word count of a Document: \", mydata['CombinedWordCountCleaned'].max())\n",
    "print(\"Mean word count of Documents: \", mydata['CombinedWordCountCleaned'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejZLE47oF2FX"
   },
   "source": [
    "### Creating a vector of word counts\n",
    "- we will use the CountVectoriser to tokenise the text and build a vocabulary of known words. \n",
    "- We first create a variable “cv” of the CountVectoriser class, and then evoke the fit_transform function to learn and build the vocabulary.\n",
    "\n",
    "###### Parameters used\n",
    "- cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "- max_df — When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). This is to ensure that we only have words relevant to the context and not commonly used words.\n",
    "- max_features — determines the number of columns in the matrix.\n",
    "- n-gram range — we would want to look at a list of single words, two words (bi-grams) and three words (tri-gram) combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gucqOlGEF2FX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(mydata['Combined description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbXm61MnF2FX"
   },
   "source": [
    "### Visualize top 20 uni-grams, bi-grams & tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1402
    },
    "id": "1UVUYZmbF2FX",
    "outputId": "0dc27188-2a81-4070-e3f4-d17d44940b76",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(mydata['Combined Description Cleaned'], n=50)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "\n",
    "#Barplot of most freq words\n",
    "sns.set(rc={'figure.figsize':(24,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1479
    },
    "id": "srWvvGBhF2FX",
    "outputId": "18140c78-acbe-425e-b4b2-98c4b41c06eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top2_words = get_top_n2_words(mydata['Combined Description Cleaned'], n=50)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "#print(top2_df)\n",
    "\n",
    "#Barplot of most freq Bi-grams\n",
    "sns.set(rc={'figure.figsize':(24,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1514
    },
    "id": "SOpMS3KCF2FX",
    "outputId": "2eee26f0-4a3d-4e1a-b403-fba65864f39c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top3_words = get_top_n3_words(mydata['Combined Description Cleaned'], n=50)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "#print(top3_df)\n",
    "#Barplot of most freq Tri-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(24,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Qi_kdJuF2FY"
   },
   "source": [
    "#### Based on the TF-IDF scores, we can extract the words with the highest scores to get the keywords for a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD2yp400F2FY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over rows with iterrows()\n",
    "doc = ' '\n",
    "for index, row in mydata.iterrows():\n",
    "     # access data using column names\n",
    "     doc = doc + row['Combined Description Cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "dsw6gRHWF2FY",
    "outputId": "79e8c440-6945-4185-8854-c7bb8104e12e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "IZ5Ara6jF2FY",
    "outputId": "94381103-7826-41d5-d46a-a8baf3d808cb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "\n",
    "#doc=mydata['Combined Description Cleaned'][0]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,50)\n",
    " \n",
    "# now print the results\n",
    "#print(\"\\nAbstract:\")\n",
    "#print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "VVyTS3mEF2FY",
    "outputId": "94756506-11a2-47a9-a25f-fe12fae07f86",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvdbM6FBF2FY"
   },
   "source": [
    "### Milestone 1 - So far we have performed these steps\n",
    "\n",
    "1. Exploring the given Data files\n",
    "2. Understanding the structure of data\n",
    "3. Missing points in data\n",
    "4. Finding inconsistencies in the data\n",
    "5. Visualizing different patterns\n",
    "6. Visualizing different text features\n",
    "7. Dealing with missing values\n",
    "8. Text preprocessing\n",
    "9. Creating word vocabulary from the corpus of report text data\n",
    "10. Creating tokens as required\n",
    "\n",
    "### Now we will run a quick model on how it performs in predicting the group with the data we have.\n",
    "### Then we will explore different data augmentation techniques (in a different notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "CoyHi_x8F2FY",
    "outputId": "499512a0-7698-4838-ba43-ddd475c70e56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEQEhFxwF2FZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mydata['Combined Description Cleaned'] = mydata['Combined Description Cleaned'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-5NdKM_F2FZ"
   },
   "source": [
    "# We will now create the preprocessed dataset required for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "1WCu8XMuF2FZ",
    "outputId": "b78d24cf-4a36-4092-9e64-84ad8ab33b70",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydata_ml = mydata[['Combined Description Cleaned', 'Assignment group']]\n",
    "mydata_ml = mydata_dl.drop(mydata_dl.index[mydata_dl[\"Combined Description Cleaned\"] == ''])\n",
    "mydata_ml = mydata_dl.reset_index(drop=True)\n",
    "mydata_ml.to_csv('drive/My Drive/datasets/input_data_after_preprocessing_for_ml.csv') \n",
    "mydata_ml.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "id": "yDWqJ0TcF2FZ",
    "outputId": "2678ca4d-f9b4-4d8b-b60a-5bc04be58f52",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "av01Ji0OF2FZ",
    "outputId": "1a0f3021-7dba-4ac6-d0ee-b1c0abbb4525",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata[mydata['Combined Description Cleaned'] == \"\"]\n",
    "empty_space.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wrDFa9gF2FZ"
   },
   "source": [
    "##### Label encode the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "llwWIhZMF2Fa",
    "outputId": "6b8c8805-e1ec-450b-ce56-8a3c4bcd3dc2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "mydata[\"LabelEncodings\"] = le.fit_transform(mydata[\"Assignment group\"])\n",
    "y_classes_len = len(le.classes_)\n",
    "le.classes_\n",
    "print(y_classes_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1FQBY-eF2Fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.asarray(mydata['LabelEncodings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "oANec8V0F2Fa",
    "outputId": "d50cb198-b5e7-49a1-daca-cbf215cbf5a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(mydata['Combined Description Cleaned'])\n",
    "print(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "wc6oPnqIF2Fa",
    "outputId": "1c909658-5353-4c18-9ed7-94bb2bd22cdd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnXKAmcxF2Fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZITWW4QuF2Fa",
    "outputId": "c8ebd0fe-d128-430b-d9ab-625229d15ce8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(count_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0zf-8ZLNF2Fb",
    "outputId": "c1cb31df-6e30-454b-fbbf-5a2fa60c4057",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tYtqMVYUF2Fb",
    "outputId": "e8fe4386-b238-449e-a414-babf37b08c67",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "oLBks21IF2Fb",
    "outputId": "87046b5b-0777-4a94-bc5c-9334b8ab9485",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class='multinomial').fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Score: \", acc_score)\n",
    "f_sc = f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "print(\"Logistic Regression F1 Score: \", f_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 48
    },
    "id": "yE-uXGNFF2Fb",
    "outputId": "dd1f096b-bef4-46a1-f3c6-4547295b1b6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "empty_space = mydata_dl[mydata_dl['Combined Description Cleaned'] == \"\"]\n",
    "empty_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb5_vNH4F2Fb",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "capstone-nlp-merged-EDA-Preprocessing-Translation-v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
