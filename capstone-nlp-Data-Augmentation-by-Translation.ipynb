{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install googletrans\n",
    "#!pip install textblob\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install -U spacy-lookups-data\n",
    "#!pip install langid\n",
    "#!pip install google_trans_new\n",
    "#!pip uninstall googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from langdetect import detect\n",
    "from itertools import cycle\n",
    "#import googletrans\n",
    "#from googletrans import Translator\n",
    "from google_trans_new import google_translator \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "#from textblob import TextBlob\n",
    "#from textblob.translate import NotTranslated\n",
    "import random\n",
    "import operator\n",
    "import math\n",
    "import tqdm\n",
    "import time\n",
    "import spacy\n",
    "import json\n",
    "import langid\n",
    "from bs4 import BeautifulSoup\n",
    "from string import digits\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section below contains  Useful Functions \n",
    "- As we find new functions, we will create them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "\n",
    "def data_augmentation_lang_translation(message, language, aug_range=1):\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "        \n",
    "    def request(lang_tgt, text):\n",
    "        t = google_translator(timeout=15)\n",
    "        translate_text = t.translate(text.strip(), lang_tgt)\n",
    "        return translate_text\n",
    "\n",
    "\n",
    "    for j in range(0,aug_range) :\n",
    "            if __name__ == \"__main__\" :\n",
    "              pool = ThreadPool(20) # Threads\n",
    "              time1 = time.time()\n",
    "              try:\n",
    "                  lang_tgt = sr.choice(language)\n",
    "                  func = partial(request, lang_tgt)\n",
    "                  results = pool.map(func, message)\n",
    "              except Exception as e:\n",
    "                  raise e\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "              time2 = time.time()\n",
    "              print(\"Translating %s Descriptions to %s, a total of %s s\"%(\"placeholder\",lang_tgt,time2 - time1))\n",
    "                \n",
    "              pool = ThreadPool(20) # Threads\n",
    "              time1 = time.time()\n",
    "              try:\n",
    "                  lang_tgt = 'en'\n",
    "                  func = partial(request, lang_tgt)\n",
    "                  results = pool.map(func, message)\n",
    "              except Exception as e:\n",
    "                  raise e\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "              time2 = time.time()\n",
    "              #print(\"Translating %s Descriptions, a total of %s s\"%(\"placeholder\",lang_tgt,time2 - time1))\n",
    "\n",
    "              augmented_messages.append(str(results))\n",
    "\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "#pool = ThreadPool(20)\n",
    "#def f(a, b, c):\n",
    "#    print(\"{} {} {}\".format(a, b, c))\n",
    "\n",
    "#def main():\n",
    "#    iterable = [1, 2, 3, 4, 5]\n",
    "#    a = \"hi\"\n",
    "#    b = \"there\"\n",
    "#    func = partial(f, a, b)\n",
    "#    pool.map(func, iterable)\n",
    "#    pool.close()\n",
    "#    pool.join()\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we will try out if we can augment data using Translation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('datasets/input_data_after_preprocessing.csv')\n",
    "#mydata = pd.read_excel(\"datasets/input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation using Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary for intent count\n",
    "## Intent is column name\n",
    "combined_description_count = mydata['Combined Description Cleaned'].value_counts().to_dict()\n",
    "combined_description_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max intent count to match other minority classes through data augmentation\n",
    "\n",
    "max_combined_description_count = max(combined_description_count.items(), key=operator.itemgetter(1))[1]\n",
    "max_combined_description_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all full descriptions\n",
    "newdf = pd.DataFrame()\n",
    "for combined_description, count in combined_description_count.items() :\n",
    "    count_diff = max_combined_description_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(mydata[mydata[\"Combined Description Cleaned\"] == combined_description][\"Combined Description Cleaned\"]) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['Combined Description Cleaned'])\n",
    "            dummy1[\"Combined Description Cleaned\"] = combined_description\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation_lang_translation(message, language, multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['Combined Description Cleaned'])\n",
    "            dummy2[\"Combined Description Cleaned\"] = combined_description\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(mydata[mydata[\"Combined Description Cleaned\"] == combined_description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stop' and don't find synonym of those words.\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = make_tokenizer(mydata['Full Description Cleaned'])    ## Message is column name\n",
    "\n",
    "X = tokenizer.texts_to_sequences(mydata['Full Description Cleaned'])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of word index\n",
    "index_word = {}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word list\n",
    "words = [value for key, value in index_word.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def most_similar(word):\n",
    "     by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "     return [w.orth_ for w in by_similarity[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synonym dictionary\n",
    "synonym_dict = {}\n",
    "\n",
    "for word in words:\n",
    "    #if (not check_oos(word)) :\n",
    "        synonym_dict.update({word : tuple([w.lower_ for w in get_word_synonym(nlp.vocab[word])])})\n",
    "        print(word, \" : \", [w.lower_ for w in get_word_synonym(nlp.vocab[word])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only consider filtered synonym\n",
    "import collections\n",
    "value_occurrences = collections.Counter(synonym_dict.values())\n",
    "\n",
    "filtered_synonym = {key: value for key, value in synonym_dict.items() if value_occurrences[value] == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary for Full Descrption \"Cleaned\" count\n",
    "## \"Full Descrption Cleaned\" is column name\n",
    "full_description_count = mydata['Full Description Cleaned'].value_counts().to_dict()\n",
    "full_description_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max \"Full Description\" count to match other minority classes through data augmentation\n",
    "#import operator\n",
    "\n",
    "max_full_description_count = max(full_description_count.items(), key=operator.itemgetter(1))[1]\n",
    "max_full_description_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all messages\n",
    "newdf = pd.DataFrame()\n",
    "for full_description, count in full_description_count.items() :\n",
    "    count_diff = max_full_description_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(mydata[mydata[\"Full Description Cleaned\"] == full_description][\"Full Description Cleaned\"]) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=[\"Full Description Cleaned\"])\n",
    "            dummy1[\"Full Description Cleaned\"] = full_description\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation_spaCy(message, multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=[\"Full Description Cleaned\"])\n",
    "            dummy2[\"Full Description Cleaned\"] = full_description\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(mydata[mydata[\"Full Description Cleaned\"] == full_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head(461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv('datasets/spaCy_Augmented_Data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/spaCy_synonyms.json', 'w') as fp:\n",
    "    json.dump(synonym_dict, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('datasets/spaCy_filtered_synonyms.json', 'w') as fp:\n",
    "    json.dump(filtered_synonym, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Augmentation using Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = make_tokenizer(mydata['Full Description Cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Dictionary of word index\n",
    "index_word = {}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word\n",
    "\n",
    "vocab_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Loading word embedding\n",
    "from time import time\n",
    "start = time()\n",
    "embed_mat = loadEmbeddingMatrix(\"glove840B300D\", vocab_dict)\n",
    "end = time()\n",
    "print(\"Embedding loaded in \", (end-start)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "synonyms_number = 5\n",
    "word_number = 20000\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=synonyms_number+1).fit(embed_mat)\n",
    "\n",
    "neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "\n",
    "synonyms = {x[0]: x[1:] for x in neighbours_mat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding nearby synonym - Basically it's not actually synonym. It's near by words of targetted word. \n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonym = {}\n",
    "for x in range(0,100):\n",
    "    try :\n",
    "        synonym.update({index_word[x] : [index_word[synonyms[x][i]] for i in range(synonyms_number-1)]})\n",
    "    except :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this synonym list to replace words with it's variation\n",
    "## Below code is in draft. But logic can be used to complete the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can only change words for selected part of speech to preserve semantic meaning.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_pos_tag (word, tagged) :\n",
    "    res = [(x, y) for x, y in tagged if x == word]\n",
    "    return res[0][1]\n",
    "\n",
    "# Load the pretrained neural net\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc in mydata[\"Full Description Cleaned\"]:\n",
    "    print(desc)\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer.tokenize(desc)\n",
    "\n",
    "    # Get the list of words from the entire text\n",
    "    words = word_tokenize(desc)\n",
    "\n",
    "    # Identify the parts of speech\n",
    "    tagged = nltk.pos_tag(words, tagset=\"universal\")\n",
    "    \n",
    "    replacements = []\n",
    "\n",
    "    for word in words:\n",
    "        synonym = []\n",
    "        antonyms = []\n",
    "        word_index = vocab_dict.get(word, None)\n",
    "\n",
    "        pos_tag = get_pos_tag(word, tagged)\n",
    "        if (word_index and pos_tag in [\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"] and word not in nltk.corpus.stopwords.words('english')) :\n",
    "            for syn in wordnet.synsets(word, eval(\"wordnet.\" + pos_tag)):\n",
    "                for l in syn.lemmas() :\n",
    "                    if(l.name() in [index_word[synonyms[word_index][i]] for i in range(synonyms_number-1)]):\n",
    "                        synonym.append(l.name())\n",
    "                    #if l.antonyms():\n",
    "                    #    antonyms.append(l.antonyms()[0].name())\n",
    "        \n",
    "        if (synonym) :\n",
    "            print(word, set(synonym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
