
1)Transformer_no_att_no_hyp- Encoder decoder without attention and no hyperparameter tuning

2)Transformer_no_att_hyp- Encoder decoder without attention and  hyperparameter tuning(learning rate and batch size)
