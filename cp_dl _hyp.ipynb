{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "ZiCE-P5OMSi5",
    "outputId": "b7663a55-ab7e-467c-eeef-938bd0b35634"
   },
   "outputs": [],
   "source": [
    "#comment out if not using colab\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pjokHDIMxvG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding,LSTM,TimeDistributed,Dense,Flatten,Dropout,RepeatVector,GRU,Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rx9PWnpTM6AX",
    "outputId": "57d86986-f094-4ad7-8df2-9029d2b0c934"
   },
   "outputs": [],
   "source": [
    "filename='pre_data_dl.xlsx'\n",
    "data_dl=pd.read_excel(filename)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data_dl.head())\n",
    "\n",
    "filename1='pre_data_dl_aug1.xlsx'\n",
    "filename2='pre_data_dl_aug2.xlsx'\n",
    "\n",
    "data_dl_aug1=pd.read_excel(filename1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data_dl_aug1.head())\n",
    "\n",
    "data_dl_aug2=pd.read_excel(filename2)\n",
    "print(data_dl_aug2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhdP0_KfNHwp"
   },
   "outputs": [],
   "source": [
    "X = (data_dl[\"Combined Description Cleaned\"])\n",
    "y= (data_dl['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y=pd.get_dummies(data_dl['Assignment group'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqd83yuiPAqX"
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "max_features=10000\n",
    "emb_dim=300\n",
    "batch_size=1024\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MG-QMXN9PSOI",
    "outputId": "0e732431-fca2-4c43-9409-fc43b1ca0846"
   },
   "outputs": [],
   "source": [
    "#function for tokenizer\n",
    "def dfTokenizer(df):\n",
    " tokenizer=Tokenizer(num_words=max_features,char_level=False)\n",
    " tokenizer.fit_on_texts(df)\n",
    " sequences=tokenizer.texts_to_sequences(df)\n",
    " return sequences,tokenizer\n",
    "#tokenization\n",
    "X,tokenizer = dfTokenizer(data_dl[\"Combined Description Cleaned\"]) \n",
    "vocab_size=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyKA-0KUQD1N"
   },
   "outputs": [],
   "source": [
    "#function for padding\n",
    "def pad(x, length=None):\n",
    " if length is None:\n",
    "   length=max([len(sentence)  for sentence in x])\n",
    " return pad_sequences(x,maxlen=length,padding='post')\n",
    " \n",
    " #padding \n",
    "X=pad_sequences(X,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbNRE75PQec1"
   },
   "outputs": [],
   "source": [
    "#function for splitting the data\n",
    "def split(X,y):\n",
    " X_train_spl,X_test_spl,y_train_spl,y_test_spl=train_test_split(X,y,test_size=0.2,random_state=123)\n",
    " return X_train_spl,X_test_spl,y_train_spl,y_test_spl\n",
    "#split the data\n",
    "X_train,X_test,y_train,y_test=split(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwcC9_SjrSQ6"
   },
   "outputs": [],
   "source": [
    "#configuring the callback\n",
    "early_stopping =  EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    min_delta=0.01, \n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcLvbfIj6-2p",
    "outputId": "0df1896a-1b07-4d3b-885a-71bfafe4c537"
   },
   "outputs": [],
   "source": [
    "#function for plotting accuracy,loss\n",
    "def plot(model,history):\n",
    " acc = history.history['accuracy']\n",
    " val_acc = history.history['val_accuracy']\n",
    " loss = history.history['loss']\n",
    " val_loss = history.history['val_loss']\n",
    " epochs = range(1, len(acc) + 1)\n",
    " plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    " plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    " plt.title(modelname + ' Training and validation accuracy')\n",
    " plt.legend()\n",
    " plt.figure()\n",
    " plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    " plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    " plt.title('Training and validation loss')\n",
    " plt.legend()\n",
    " plt.show()\n",
    " return \n",
    "inp_len=X.shape[1]\n",
    "print(inp_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZxJb6ugQp-j"
   },
   "outputs": [],
   "source": [
    "#Simple LSTM model\n",
    "def LSTM_model(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len))\n",
    " model.add(LSTM(512, return_sequences=True))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    " \n",
    "#Using GRU\n",
    "def GRU_model(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len))\n",
    " model.add(GRU(512, return_sequences=True))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model    \n",
    "\n",
    "#BILSTM\n",
    "def BILSTM_model(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len))\n",
    " model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "#Using bidirectional GRU\n",
    "def BIGRU_model(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len))\n",
    " model.add(Bidirectional(GRU(512, return_sequences=True)))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "#using transformer(no of layers in transfomers have been reduce as the total number of parameters reaches close to 400 million when using 512 layers)\n",
    "def TRANSFORMER_model(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len))\n",
    " model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(RepeatVector(max_features))\n",
    " model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    " model.add(TimeDistributed(Dense(units = 128, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])   \n",
    " model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    " return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZxJb6ugQp-j"
   },
   "outputs": [],
   "source": [
    "#customising the kerastuner function to include batch size and epochs\n",
    "class MyTuner(kt.tuners.BayesianOptimization):\n",
    "  def run_trial(self, trial, *args, **kwargs):\n",
    "     kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 256, 512,step=256 )\n",
    "     kwargs['epochs'] = trial.hyperparameters.Int('epochs', 5, 10,20)\n",
    "     super(MyTuner, self).run_trial(trial, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AlELfXNMvHj4",
    "outputId": "2889f402-2598-49b3-f808-cb07cbea18e7"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    LSTM_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/1\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary5())\n",
    "best_model_LSTM = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for LSTM are\",best_model_LSTM)\n",
    "\n",
    "\n",
    "modelname=\"LSTMModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_LSTM.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_LSTM,history)\n",
    "\n",
    "scores_LSTM =best_model_LSTM.evaluate(X_test, y_test, verbose=0)\n",
    "scores_LSTM_val = best_model_LSTM.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of LSTM for unaugmented data is :\", (scores_LSTM[1]*100))\n",
    "print(\"Validation Accuracy of LSTM for unagumented data is:\", (scores_LSTM_val[1]*100))\n",
    "#print(\"Validation Accuracy of LSTM for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of LSTM for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of LSTM for unaugmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kUE4Fla4vktV",
    "outputId": "8780f068-fa52-4d5c-8389-1ac88e3f341d"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    GRU_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/2\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_GRU = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for GRU are\",best_model_GRU)\n",
    "\n",
    "\n",
    "modelname=\"GRUModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_GRU.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_GRU,history)\n",
    "\n",
    "scores_GRU =best_model_GRU.evaluate(X_test, y_test, verbose=0)\n",
    "scores_GRU_val = best_model_GRU.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of GRU for unaugmented data is :\", (scores_GRU[1]*100))\n",
    "print(\"Validation Accuracy of GRU for unagumented data is:\", (scores_GRU_val[1]*100))\n",
    "#print(\"Validation Accuracy of GRU for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of GRU for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of GRU for unaugmented data is :\",np.array(history.history['loss']).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lb2m8_ocvlWZ",
    "outputId": "45889635-1876-422d-dd44-ef8257a05347"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    BILSTM_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/3\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_BILSTM = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for BILSTM are\",best_model_BILSTM)\n",
    "\n",
    "\n",
    "modelname=\"BILSTMModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_BILSTM.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_BILSTM,history)\n",
    "\n",
    "scores_BILSTM =best_model_BILSTM.evaluate(X_test, y_test, verbose=0)\n",
    "scores_BILSTM_val = best_model_BILSTM.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of BILSTM for unaugmented data is :\", (scores_BILSTM[1]*100))\n",
    "print(\"Validation Accuracy of BILSTM for unagumented data is:\", (scores_BILSTM_val[1]*100))\n",
    "#print(\"Validation Accuracy of BILSTM for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of BILSTM for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of BILSTM for unaugmented data is :\",np.array(history.history['loss']).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W7j9MWsavlOs",
    "outputId": "c01749f6-8181-4cc8-ec0d-2154d1913bfb"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    BIGRU_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/4\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_BIGRU = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for BIGRU are\",best_model_BIGRU)\n",
    "\n",
    "\n",
    "modelname=\"BIGRUModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_BIGRU.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_BIGRU,history)\n",
    "\n",
    "scores_BIGRU =best_model_BIGRU.evaluate(X_test, y_test, verbose=0)\n",
    "scores_BIGRU_val = best_model_BIGRU.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of BIGRU for unaugmented data is :\", (scores_BIGRU[1]*100))\n",
    "print(\"Validation Accuracy of BIGRU for unagumented data is:\", (scores_BIGRU_val[1]*100))\n",
    "#print(\"Validation Accuracy of BIGRU for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of BIGRU for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of BIGRU for unaugmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W7j9MWsavlOs",
    "outputId": "c01749f6-8181-4cc8-ec0d-2154d1913bfb"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    TRANSFORMER_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/4A\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_TRANSFORMER = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for TRANSFORMER are\",best_model_TRANSFORMER)\n",
    "\n",
    "\n",
    "modelname=\"TRANSFORMERModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_TRANSFORMER.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_TRANSFORMER,history)\n",
    "\n",
    "scores_TRANSFORMER =best_model_TRANSFORMER.evaluate(X_test, y_test, verbose=0)\n",
    "scores_TRANSFORMER_val = best_model_TRANSFORMER.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of TRANSFORMER for unaugmented data is :\", (scores_TRANSFORMER[1]*100))\n",
    "print(\"Validation Accuracy of TRANSFORMER for unagumented data is:\", (scores_TRANSFORMER_val[1]*100))\n",
    "#print(\"Validation Accuracy of TRANSFORMER for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of TRANSFORMER for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of TRANSFORMER for unaugmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmoJ5DUvt8lD"
   },
   "outputs": [],
   "source": [
    "X_aug1 = (data_dl_aug1[\"Combined Description Cleaned\"])\n",
    "y_aug1= (data_dl_aug1['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y_aug1=pd.get_dummies(data_dl_aug2['Assignment group'])\n",
    "\n",
    "\n",
    "X_aug2 = (data_dl_aug2[\"Combined Description Cleaned\"])\n",
    "y_aug2= (data_dl_aug2['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y_aug2=pd.get_dummies(data_dl_aug2['Assignment group'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvl4V_67uKb_",
    "outputId": "23d0507a-d29b-473e-9474-ec96440bf194"
   },
   "outputs": [],
   "source": [
    "#tokenization\n",
    "X_aug1,tokenizer = dfTokenizer(data_dl_aug1[\"Combined Description Cleaned\"]) \n",
    "vocab_size_aug1=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size_aug1)\n",
    "\n",
    "X_aug2,tokenizer = dfTokenizer(data_dl_aug2[\"Combined Description Cleaned\"]) \n",
    "vocab_size_aug2=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size_aug2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2n9EEXduMGz"
   },
   "outputs": [],
   "source": [
    "#padding\n",
    "X_aug1=pad_sequences(X_aug1,padding='post')\n",
    "X_aug2=pad_sequences(X_aug2,padding='post')\n",
    "y_aug1=y_aug1[0:17586]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4hMuuz0uQUD",
    "outputId": "3e84b829-cccb-44c4-c0d7-864feddd7a63"
   },
   "outputs": [],
   "source": [
    "#split the data.Only augmented data is split using stratify\n",
    "def split_stratify(X,y):\n",
    " X_train_spl,X_test_spl,y_train_spl,y_test_spl=train_test_split(X,y,test_size=0.2,stratify=y,random_state=123)\n",
    " return X_train_spl,X_test_spl,y_train_spl,y_test_spl\n",
    "\n",
    "X_train_aug1,X_test_aug1,y_train_aug1,y_test_aug1=split_stratify(X_aug1,y_aug1)\n",
    "X_train_aug2,X_test_aug2,y_train_aug2,y_test_aug2=split_stratify(X_aug2,y_aug2)\n",
    "\n",
    "\n",
    "inp_len1=X_aug1.shape[1]\n",
    "print(inp_len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfd57Hy8iTjx"
   },
   "outputs": [],
   "source": [
    "#Simple LSTM model1\n",
    "def LSTM_model1(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len1))\n",
    " model.add(LSTM(512, return_sequences=True))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "\n",
    "\n",
    "#Using GRU\n",
    "def GRU_model1(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len1))\n",
    " model.add(GRU(512, return_sequences=True))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "\n",
    " #Using bidirectional LSTM\n",
    "\n",
    "def BILSTM_model1(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len1))\n",
    " model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "\n",
    "#Using bidirectional GRU\n",
    "def BIGRU_model1(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len1))\n",
    " model.add(Bidirectional(GRU(512, return_sequences=True)))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(TimeDistributed(Dense(units = 512, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    " model.compile(optimizer=Adam(lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "\n",
    "#using transformer(no of layers in transfomers have been reduce as the total number of parameters reaches close to 400 million when using 512 layers)\n",
    "def TRANSFORMER_model1(hp):\n",
    " model = tf.keras.Sequential()\n",
    " model.add(Embedding(max_features,emb_dim,input_length=inp_len1))\n",
    " model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    " model.add(Dropout(0.2))\n",
    " model.add(RepeatVector(max_features))\n",
    " model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    " model.add(TimeDistributed(Dense(units = 128, activation = 'relu')))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(74, activation='softmax'))\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])   \n",
    " model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d47XGO6FuUMU",
    "outputId": "db0930bb-d520-4a1e-bb6c-e92a058d8b1b"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    LSTM_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/5\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_LSTM = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for LSTM are\",best_model_LSTM)\n",
    "\n",
    "\n",
    "modelname=\"LSTMModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_LSTM.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_LSTM,history)\n",
    "\n",
    "scores_LSTM_aug1 =best_model_LSTM.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_LSTM_val_aug1 = best_model_LSTM.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of LSTM for level1 augmented data is :\", (scores_LSTM_aug1[1]*100))\n",
    "print(\"Validation Accuracy of LSTM for level1 augmented data is:\", (scores_LSTM_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of LSTM for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of LSTM for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of LSTM for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tSq6DbzHuZTd",
    "outputId": "e5507bb8-0944-4ee5-b492-0af913e3f1ef"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    GRU_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/6\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_GRU = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for GRU are\",best_model_GRU)\n",
    "\n",
    "\n",
    "modelname=\"GRUModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_GRU.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_GRU,history)\n",
    "\n",
    "scores_GRU_aug1 =best_model_GRU.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_GRU_val_aug1 = best_model_GRU.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of GRU for level1 augmented data is :\", (scores_GRU_aug1[1]*100))\n",
    "print(\"Validation Accuracy of GRU for level1 augmented data is:\", (scores_GRU_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of GRU for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of GRU for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of GRU for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1250
    },
    "id": "47BBsqxwufxo",
    "outputId": "0a5fbe19-65a5-473a-e220-71a365bf4bb4"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    BILSTM_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/7\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_BILSTM = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for BILSTM are\",best_model_BILSTM)\n",
    "\n",
    "\n",
    "modelname=\"BILSTMModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_BILSTM.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_BILSTM,history)\n",
    "\n",
    "scores_BILSTM_aug1 =best_model_BILSTM.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_BILSTM_val_aug1 = best_model_BILSTM.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of BILSTM for level1 augmented data is :\", (scores_BILSTM_aug1[1]*100))\n",
    "print(\"Validation Accuracy of BILSTM for level1 augmented  data is:\", (scores_BILSTM_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of BILSTM forlevel1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of BILSTM for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of BILSTM for level1 augmented data is :\",np.array(history.history['loss']).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1250
    },
    "id": "EbZurK_LuhSw",
    "outputId": "47543f6f-b9b4-44f4-e299-97ee9ffac7a1"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    BIGRU_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/8\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_BIGRU = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for BIGRU are\",best_model_BIGRU)\n",
    "\n",
    "\n",
    "modelname=\"BIGRUModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_BIGRU.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_BIGRU,history)\n",
    "\n",
    "scores_BIGRU_aug1 =best_model_BIGRU.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_BIGRU_val_aug1 = best_model_BIGRU.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of BIGRU for level1 augmented data is :\", (scores_BIGRU_aug1[1]*100))\n",
    "print(\"Validation Accuracy of BIGRU for level1 augmented data is:\", (scores_BIGRU_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of BIGRU for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of BIGRU for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of BIGRU for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1250
    },
    "id": "EbZurK_LuhSw",
    "outputId": "47543f6f-b9b4-44f4-e299-97ee9ffac7a1"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    TRANSFORMER_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/8A\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_TRANSFORMER = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for TRANSFORMER are\",best_model_TRANSFORMER)\n",
    "\n",
    "\n",
    "modelname=\"TRANSFORMERModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_TRANSFORMER.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_TRANSFORMER,history)\n",
    "\n",
    "scores_TRANSFORMER_aug1 =best_model_TRANSFORMER.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_TRANSFORMER_val_aug1 = best_model_TRANSFORMER.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of TRANSFORMER for level1 augmented data is :\", (scores_TRANSFORMER_aug1[1]*100))\n",
    "print(\"Validation Accuracy of TRANSFORMER for level1 augmented data is:\", (scores_TRANSFORMER_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of TRANSFORMER for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of TRANSFORMER for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of TRANSFORMER for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YXzWNefFuo9c",
    "outputId": "2b120a00-4205-47b6-a37a-ec88d87a70e9"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    LSTM_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_LSTM = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for LSTM are\",best_model_LSTM)\n",
    "\n",
    "\n",
    "modelname=\"LSTMModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_LSTM.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_LSTM,history)\n",
    "\n",
    "scores_LSTM_aug2 =best_model_LSTM.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_LSTM_val_aug2 = best_model_LSTM.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of LSTM for level2 augmented data is :\", (scores_LSTM_aug2[1]*100))\n",
    "print(\"Validation Accuracy of LSTM for level2 augmented data is:\", (scores_LSTM_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of LSTM for level2 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of LSTM for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of LSTM for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S0mvWiW9uuHB",
    "outputId": "2df76808-fce2-4780-f947-649b87508f46"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    GRU_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/10\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_GRU = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for GRU are\",best_model_GRU)\n",
    "\n",
    "\n",
    "modelname=\"GRUModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_GRU.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_GRU,history)\n",
    "\n",
    "scores_GRU_aug2 =best_model_GRU.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_GRU_val_aug2 = best_model_GRU.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of GRU for level2 augmented data is :\", (scores_GRU_aug2[1]*100))\n",
    "print(\"Validation Accuracy of GRU for level2 augmented data is:\", (scores_GRU_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of GRU for level2 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of GRU for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of GRU for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDu5IbHIuxID",
    "outputId": "43bc2782-9cef-4fca-916d-23f543989636"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    BILSTM_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/11\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_BILSTM = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for BILSTM are\",best_model_BILSTM)\n",
    "\n",
    "\n",
    "modelname=\"BILSTMModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_BILSTM.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_BILSTM,history)\n",
    "\n",
    "scores_BILSTM_aug2 =best_model_BILSTM.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_BILSTM_val_aug2 = best_model_BILSTM.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of BILSTM for level2 augmented data is :\", (scores_BILSTM_aug2[1]*100))\n",
    "print(\"Validation Accuracy of BILSTM for level2 augmented  data is:\", (scores_BILSTM_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of BILSTM forlevel2 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of BILSTM for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of BILSTM for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CQwAShGu6_5"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    BIGRU_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/12\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_BIGRU = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for BIGRU are\",best_model_BIGRU)\n",
    "\n",
    "\n",
    "modelname=\"BIGRUModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_BIGRU.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_BIGRU,history)\n",
    "\n",
    "scores_BIGRU_aug2 =best_model_BIGRU.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_BIGRU_val_aug2 = best_model_BIGRU.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of BIGRU for level2 augmented data is :\", (scores_BIGRU_aug2[1]*100))\n",
    "print(\"Validation Accuracy of BIGRU for level2 augmented data is:\", (scores_BIGRU_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of BIGRU for level2 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of BIGRU for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of BIGRU for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CQwAShGu6_5"
   },
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    TRANSFORMER_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/12A\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_TRANSFORMER = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for TRANSFORMER are\",best_model_TRANSFORMER)\n",
    "\n",
    "\n",
    "modelname=\"TRANSFORMERModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_TRANSFORMER.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_TRANSFORMER,history)\n",
    "\n",
    "scores_TRANSFORMER_aug2 =best_model_TRANSFORMER.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_TRANSFORMER_val_aug2 = best_model_TRANSFORMER.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of TRANSFORMER for level2 augmented data is :\", (scores_TRANSFORMER_aug2[1]*100))\n",
    "print(\"Validation Accuracy of TRANSFORMER for level2 augmented data is:\", (scores_TRANSFORMER_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of TRANSFORMER for level2 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of TRANSFORMER for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of TRANSFORMER for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QR-8idrvCk6"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of LSTM with hyperparameter tuning for unagumented data is:\", (scores_LSTM[1]*100))\n",
    "print(\"Accuracy of GRU with hyperparameter tuning for unagumented data is:\",  (scores_GRU[1]*100))\n",
    "print(\"Accuracy of BILSTM with hyperparameter tuning unagumented data is:\", (scores_BILSTM[1]*100))\n",
    "print(\"Accuracy of BIGRU with hyperparameter tuning unagumented data is:\", (scores_BIGRU[1]*100))\n",
    "print(\"Accuracy of ENCODER DECODER without attention with hyperparameter tuning unagumented data is:\", (scores_TRANSFORMER[1]*100))\n",
    "\n",
    "print(\"Accuracy of LSTM with hyperparameter tuning for level1 augmentation data is:\", (scores_LSTM_aug1[1]*100))\n",
    "print(\"Accuracy of GRU with hyperparameter tuning for level1 augmentation data is:\", (scores_GRU_aug1[1]*100))\n",
    "print(\"Accuracy of BILSTM with hyperparameter tuning for level1 augmentation data is:\", (scores_BILSTM_aug1[1]*100))\n",
    "print(\"Accuracy of BIGRU with hyperparameter tuning for level1 augmentation data is:\", (scores_BIGRU_aug1[1]*100))\n",
    "print(\"Accuracy of ENCODER DECODER without attention with hyperparameter tuning for level1 augmentation data is:\", (scores_TRANSFORMER_aug1[1]*100))\n",
    "\n",
    "\n",
    "print(\"Accuracy of LSTM with hyperparameter tuning for level2 augmentation data is:\", (scores_LSTM_aug2[1]*100))\n",
    "print(\"Accuracy of GRU with hyperparameter tuning for level2 augmentation data is:\", (scores_GRU_aug2[1]*100))\n",
    "print(\"Accuracy of BILSTM with hyperparameter tuning for level2 augmentation data is:\", (scores_BILSTM_aug2[1]*100))\n",
    "print(\"Accuracy of BIGRU with hyperparameter tuning for level2 augmentation data is:\", (scores_BIGRU_aug2[1]*100))\n",
    "print(\"Accuracy of ENCODER DECODER without attention with hyperparameter tuning for level2 augmentation data is:\", (scores_TRANSFORMER_aug2[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8K-LGoiB6cH2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cp_dl.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
